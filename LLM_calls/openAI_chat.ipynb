{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T06:50:40.029971Z",
     "start_time": "2024-01-14T06:50:40.019882Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化环境\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = '${YOUR_API_KEY}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bab9e7181fa24e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-14T06:50:43.367844Z",
     "start_time": "2024-01-14T06:50:42.506310Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-9ErR52JmnfipEu4QVdSN1ThNO39ng\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Hello\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}],\"created\":1713329027,\"model\":\"gpt-3.5-turbo-1106\",\"object\":\"chat.completion\",\"system_fingerprint\":\"fp_77a673219d\",\"usage\":{\"completion_tokens\":1,\"prompt_tokens\":20,\"total_tokens\":21}}\n"
     ]
    }
   ],
   "source": [
    "# 一般参数\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import completion_create_params\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! \"}\n",
    "    ],\n",
    "    n=1,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.8,\n",
    "    # top_p=1,\n",
    "    stop=\"!\",\n",
    "    # seed=1234567,\n",
    "    user=\"user_1234567\",\n",
    ")\n",
    "\n",
    "print(completion.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9387f85fb440a34c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T15:36:28.124150Z",
     "start_time": "2023-12-30T15:36:27.087690Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\":\"{\\n  \\\"winner\\\": \\\"Los Angeles Dodgers\\\"\\n}\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}\n"
     ]
    }
   ],
   "source": [
    "# response_format 参数 https://platform.openai.com/docs/guides/text-generation/json-mode\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import completion_create_params\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "    ],\n",
    "    # 对content进行json格式化\n",
    "    response_format=completion_create_params.ResponseFormat(type=\"json_object\")\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ef683",
   "metadata": {},
   "source": [
    "response_format 参数 https://platform.openai.com/docs/guides/text-generation/json-mode\n",
    "如果想使用response_type为json格式，必须在prompt中声明‘json’字段，否则报错：\n",
    "`'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'`\n",
    "这是个反例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad0b2a53-edf1-4d8a-9b13-a54c77c6082b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T15:36:28.124150Z",
     "start_time": "2023-12-30T15:36:27.087690Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'. (request id: 20240416163749766878375Ak7gj9S1)\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m completion_create_params\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m----> 7\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-1106\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho won the world series in 2020?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 对content进行json格式化\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson_object\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mmodel_dump_json())\n",
      "File \u001b[0;32m~/miniconda3/envs/openai_api_python/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openai_api_python/lib/python3.11/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openai_api_python/lib/python3.11/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/openai_api_python/lib/python3.11/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openai_api_python/lib/python3.11/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    959\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    963\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    964\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    968\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'. (request id: 20240416163749766878375Ak7gj9S1)\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.chat import completion_create_params\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "    ],\n",
    "    # 对content进行json格式化\n",
    "    response_format=completion_create_params.ResponseFormat(type=\"json_object\")\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5ec76",
   "metadata": {},
   "source": [
    "stream 是一个一个蹦的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "516bdf352343e73f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T13:14:48.519855Z",
     "start_time": "2023-12-27T13:14:48.462304Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\":\"\",\"function_call\":null,\"role\":\"assistant\",\"tool_calls\":null}\n",
      "{\"content\":\"Hello\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\"!\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\" How\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\" can\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\" I\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\" assist\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\" you\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\" today\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":\"?\",\"function_call\":null,\"role\":null,\"tool_calls\":null}\n",
      "{\"content\":null,\"function_call\":null,\"role\":null,\"tool_calls\":null}\n"
     ]
    }
   ],
   "source": [
    "# stream流式输出\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed7fc0f026699c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-28T15:09:41.212810Z",
     "start_time": "2023-12-28T15:09:38.541734Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906]\n",
      "{\"content\":\"Hi there! How can I assist you today?\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}\n"
     ]
    }
   ],
   "source": [
    "# logit_bias参数 [-100, 100], 100表示必选，-100 表示禁止\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"Hello\"))\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ],\n",
    "    logit_bias={enc.encode(\"Hello\")[0]: -100} # 这里要注意一个点：key是token\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eada43c111acf25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T13:26:37.059608Z",
     "start_time": "2023-12-27T13:26:34.817456Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\":\"Hello! How can I assist you today?\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}\n",
      "{\"content\":[{\"token\":\"Hello\",\"bytes\":[72,101,108,108,111],\"logprob\":-0.21570958,\"top_logprobs\":[{\"token\":\"Hello\",\"bytes\":[72,101,108,108,111],\"logprob\":-0.21570958},{\"token\":\"Hi\",\"bytes\":[72,105],\"logprob\":-1.6720008}]},{\"token\":\"!\",\"bytes\":[33],\"logprob\":-0.20926832,\"top_logprobs\":[{\"token\":\"!\",\"bytes\":[33],\"logprob\":-0.20926832},{\"token\":\" there\",\"bytes\":[32,116,104,101,114,101],\"logprob\":-1.6910303}]},{\"token\":\" How\",\"bytes\":[32,72,111,119],\"logprob\":-0.00095136015,\"top_logprobs\":[{\"token\":\" How\",\"bytes\":[32,72,111,119],\"logprob\":-0.00095136015},{\"token\":\" Is\",\"bytes\":[32,73,115],\"logprob\":-7.526584}]},{\"token\":\" can\",\"bytes\":[32,99,97,110],\"logprob\":-0.014271167,\"top_logprobs\":[{\"token\":\" can\",\"bytes\":[32,99,97,110],\"logprob\":-0.014271167},{\"token\":\" are\",\"bytes\":[32,97,114,101],\"logprob\":-4.673554}]},{\"token\":\" I\",\"bytes\":[32,73],\"logprob\":-0.00001700133,\"top_logprobs\":[{\"token\":\" I\",\"bytes\":[32,73],\"logprob\":-0.00001700133},{\"token\":\" i\",\"bytes\":[32,105],\"logprob\":-11.576181}]},{\"token\":\" assist\",\"bytes\":[32,97,115,115,105,115,116],\"logprob\":-0.32928452,\"top_logprobs\":[{\"token\":\" assist\",\"bytes\":[32,97,115,115,105,115,116],\"logprob\":-0.32928452},{\"token\":\" help\",\"bytes\":[32,104,101,108,112],\"logprob\":-1.2748374}]},{\"token\":\" you\",\"bytes\":[32,121,111,117],\"logprob\":-0.000036789137,\"top_logprobs\":[{\"token\":\" you\",\"bytes\":[32,121,111,117],\"logprob\":-0.000036789137},{\"token\":\" or\",\"bytes\":[32,111,114],\"logprob\":-10.491168}]},{\"token\":\" today\",\"bytes\":[32,116,111,100,97,121],\"logprob\":-0.0055119265,\"top_logprobs\":[{\"token\":\" today\",\"bytes\":[32,116,111,100,97,121],\"logprob\":-0.0055119265},{\"token\":\"?\",\"bytes\":[63],\"logprob\":-5.204969}]},{\"token\":\"?\",\"bytes\":[63],\"logprob\":-0.00003333223,\"top_logprobs\":[{\"token\":\"?\",\"bytes\":[63],\"logprob\":-0.00003333223},{\"token\":\"?\\n\",\"bytes\":[63,10],\"logprob\":-10.532458}]}]}\n"
     ]
    }
   ],
   "source": [
    "# logprobs参数\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ],\n",
    "    logprobs=True,\n",
    "    top_logprobs=2\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.model_dump_json())\n",
    "print(completion.choices[0].logprobs.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0a5776",
   "metadata": {},
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c6aa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first call: {\"id\":\"chatcmpl-9ErRRLw7IbrkWFODggnUzwk99QkFR\",\"choices\":[{\"finish_reason\":\"tool_calls\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":[{\"id\":\"call_ToIgUbw4UcZhi6dyAyCPyf0v\",\"function\":{\"arguments\":\"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"celsius\\\"}\",\"name\":\"get_current_weather\"},\"type\":\"function\"},{\"id\":\"call_XrMEOIxoy6JUSoSJ1DuQuQrc\",\"function\":{\"arguments\":\"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\",\"name\":\"get_current_weather\"},\"type\":\"function\"}]}}],\"created\":1713329049,\"model\":\"gpt-3.5-turbo-1106\",\"object\":\"chat.completion\",\"system_fingerprint\":\"fp_ad2b9c6e11\",\"usage\":{\"completion_tokens\":59,\"prompt_tokens\":89,\"total_tokens\":148}}\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义函数\n",
    "# 1.1 定义模拟获取天气的本地函数\n",
    "def get_current_weather(location, unit):\n",
    "    # Call the weather API\n",
    "    return f\"It's 20 {unit} in {location}\"\n",
    "\n",
    "\n",
    "# 1.2 定义函数字典方便调用\n",
    "function_dict = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "}\n",
    "\n",
    "# 1.3 定义chat接口需要的函数\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in Boston today with celsius and fahrenheit?\"}]\n",
    "\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    ")\n",
    "\n",
    "print(f\"first call: {completion.model_dump_json()}\") # 2. 第一次调用chat接口，返回的是函数调用的提示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e27a2",
   "metadata": {},
   "source": [
    "response中从传入的tools中“智能”得选出了应该调用的函数\n",
    "```\n",
    "\"tool_calls\": [{\n",
    "\t\t\t\t\"id\": \"call_gUDAqeorgDQjWJtvt0sxevcl\",\n",
    "\t\t\t\t\"function\": {\n",
    "\t\t\t\t\t\"arguments\": \"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"celsius\\\"}\",\n",
    "\t\t\t\t\t\"name\": \"get_current_weather\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"type\": \"function\"\n",
    "\t\t\t}, {\n",
    "\t\t\t\t\"id\": \"call_lZEhwk2r2HliYy54ZFBhdCpU\",\n",
    "\t\t\t\t\"function\": {\n",
    "\t\t\t\t\t\"arguments\": \"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\",\n",
    "\t\t\t\t\t\"name\": \"get_current_weather\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"type\": \"function\"\n",
    "\t\t\t}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bf4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second call massages: [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today with celsius and fahrenheit?\"}, {\"content\": null, \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": [{\"id\": \"call_ToIgUbw4UcZhi6dyAyCPyf0v\", \"function\": {\"arguments\": \"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"celsius\\\"}\", \"name\": \"get_current_weather\"}, \"type\": \"function\"}, {\"id\": \"call_XrMEOIxoy6JUSoSJ1DuQuQrc\", \"function\": {\"arguments\": \"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\", \"name\": \"get_current_weather\"}, \"type\": \"function\"}]}, {\"role\": \"tool\", \"tool_call_id\": \"call_ToIgUbw4UcZhi6dyAyCPyf0v\", \"content\": \"It's 20 celsius in Boston, MA\"}, {\"role\": \"tool\", \"tool_call_id\": \"call_XrMEOIxoy6JUSoSJ1DuQuQrc\", \"content\": \"It's 20 fahrenheit in Boston, MA\"}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 3. 从结果接口的结果中获取函数调用的参数 进行本地函数调用\n",
    "# 3.1 获取函数调用的参数\n",
    "response_message = completion.choices[0].message\n",
    "messages.append(response_message)   # 必须要传\n",
    "# 遍历tool_calls，获取函数调用的id，函数名，函数参数\n",
    "for tool_call in response_message.tool_calls:\n",
    "    tool_id = tool_call.id\n",
    "    function_name = tool_call.function.name\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    # 3.2 调用本地函数\n",
    "    function_response = function_dict.get(function_name)(**function_args)\n",
    "    # 3.3 将本地函数的结果作为chat接口的输入\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_id,\n",
    "        \"content\": function_response,\n",
    "    })\n",
    "\n",
    "def custom_encoder(obj):\n",
    "    if hasattr(obj, 'model_dump'):\n",
    "        return obj.model_dump()  # 调用自定义的 to_dict 方法\n",
    "    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "print(f\"second call massages: {json.dumps(messages, default=custom_encoder,ensure_ascii=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3ed090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second call: {\"id\":\"chatcmpl-9ErSyoQdo5sFRvXDy2nATr13qrEtA\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"The weather in Boston today is 20 degrees Celsius (68 degrees Fahrenheit).\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}],\"created\":1713329144,\"model\":\"gpt-3.5-turbo-1106\",\"object\":\"chat.completion\",\"system_fingerprint\":\"fp_ad2b9c6e11\",\"usage\":{\"completion_tokens\":15,\"prompt_tokens\":115,\"total_tokens\":130}}\n"
     ]
    }
   ],
   "source": [
    "# 4. 第二次调用chat接口，返回的是chat的最终结果\n",
    "completion_final = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=messages,\n",
    ")\n",
    "print(f\"second call: {completion_final.model_dump_json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87115f",
   "metadata": {},
   "source": [
    "小结tool\n",
    "tools的使用会调用OpenAI两次，第一次将本地函数、需求传入，返回要调用的函数(可能是多个，后面可以遍历调用)；本地逐一调用函数并将结果与第一步返回的函数列表(包含`tool_calls`的外层)一起传入openAI，让其组装最后的结果。这里必须要传tool_calls的外层给openAI，不然它不知道如何组装本地函数调用结果，会提示你要带着tool_calls: `Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eaf5f6",
   "metadata": {},
   "source": [
    "vision\n",
    "将视觉能力融合到自己的场景中，例如图像识别、智能监控等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second call: {\"id\":\"chatcmpl-9ErSyoQdo5sFRvXDy2nATr13qrEtA\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"The weather in Boston today is 20 degrees Celsius (68 degrees Fahrenheit).\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}],\"created\":1713329144,\"model\":\"gpt-3.5-turbo-1106\",\"object\":\"chat.completion\",\"system_fingerprint\":\"fp_ad2b9c6e11\",\"usage\":{\"completion_tokens\":15,\"prompt_tokens\":115,\"total_tokens\":130}}\n"
     ]
    }
   ],
   "source": [
    "# 4. 第二次调用chat接口，返回的是chat的最终结果\n",
    "completion_final = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=messages,\n",
    ")\n",
    "print(f\"second call: {completion_final.model_dump_json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d427b98061dc80a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T03:44:29.547917Z",
     "start_time": "2023-12-30T03:44:22.900420Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"The image shows a beautiful natural landscape with a wooden boardwalk extending straight into the distance. The boardwalk is surrounded by lush green grasses and foliage. The sky is bright and partly cloudy, suggesting a sunny day with good weather. In the distance, you can see a line of trees and some shrubbery that demarcates the end of the open field. This looks like a peaceful setting for a walk or to enjoy nature.\",\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0].model_dump_json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a680f9",
   "metadata": {},
   "source": [
    "[后记]api中注意到的参数：\n",
    "\n",
    "* frequency_penalty，频率惩罚，[-2,2]，负值时会奖励重复，正值会惩罚重复。\n",
    "\n",
    "* presence_penalty，[-2,2]，是否重复使用之前生成的词汇，正值会惩罚新token，即正值会唠叨，负值会更灵活。若想就围绕一个话题使劲地讨论 -2，若想避免重复讨论 2。\n",
    "\n",
    "* logit_bias 增加或减少某个token出现的概率，-100 禁止 100必选。\n",
    "\n",
    "* response_format 我上次在这里踩了坑：单纯指定这个参数不行的，需要在prompt中声明json，API文档中也强调了这点(不声明的话报错：'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object')。\n",
    "\n",
    "  > 忘记之前的运行效果了，这次试的效果是：response_type可以不指定 prompt中指定即可；但response_type指定了，prompt必须指定json。\n",
    "\n",
    "* seed，和SD中的seed类同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
